@incollection{test,
  doi = {10.1007/978-3-030-77726-5_3},
  year = {2022},
  publisher = {Springer International Publishing},
  pages = {51--83},
  author = {Martina Mara and Kathrin Meyer},
  title = {Acceptance of Autonomous Vehicles: An Overview of User-Specific,  Car-Specific and Contextual Determinants},
  booktitle = {Studies in Computational Intelligence}
}

@article{Kosti_2019,
   title={Context Based Emotion Recognition using EMOTIC Dataset},
   ISSN={1939-3539},
   url={http://dx.doi.org/10.1109/TPAMI.2019.2916866},
   DOI={10.1109/tpami.2019.2916866},
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Kosti, Ronak and Alvarez, Jose and Recasens, Adria and Lapedriza, Agata},
   year={2019},
   pages={1–1} }

@misc{mittal2020emoticon,
      title={EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege's Principle}, 
      author={Trisha Mittal and Pooja Guhan and Uttaran Bhattacharya and Rohan Chandra and Aniket Bera and Dinesh Manocha},
      year={2020},
      eprint={2003.06692},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{mollahosseini2017affectnet,
  title={AffectNet: A New Database for Facial Expression, Valence, and Arousal Computation in the Wild},
  author={Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
  journal={IEEE Transactions on Affective Computing},
  year={2017},
  publisher={IEEE}
}

@article{emotic_pami2019,
  title={Context based emotion recognition using emotic dataset},
  author={Kosti, Ronak and Alvarez, Jose and Recasens, Adria and Lapedriza, Agata},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2019},
  publisher={IEEE}
}

@article{EkoAgus, 
title = {Evaluating perceived safety of autonomous vehicle: The influence of privacy and cybersecurity to cognitive and emotional safety}
,
journal = {IATSS Research},
volume = {47},
number = {2},
pages = {160-170},
year = {2023},
issn = {0386-1112},
doi = {https://doi.org/10.1016/j.iatssr.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0386111223000262},
author = {Eko Agus Prasetio and Cintia Nurliyana},
keywords = {Autonomous vehicle, Cognitive, Cyber security, Emotional, Perceived safety},
}

@article{rusellmodell,
author = {Russell, James},
year = {1980},
month = {12},
pages = {1161-1178},
title = {A Circumplex Model of Affect},
volume = {39},
journal = {Journal of Personality and Social Psychology},
doi = {10.1037/h0077714}
}

@Article{electronics12173595,
AUTHOR = {Zhang, Saining and Zhang, Yuhang and Zhang, Ye and Wang, Yufei and Song, Zhigang},
TITLE = {A Dual-Direction Attention Mixed Feature Network for Facial Expression Recognition},
JOURNAL = {Electronics},
VOLUME = {12},
YEAR = {2023},
NUMBER = {17},
ARTICLE-NUMBER = {3595},
URL = {https://www.mdpi.com/2079-9292/12/17/3595},
ISSN = {2079-9292},
ABSTRACT = {In recent years, facial expression recognition (FER) has garnered significant attention within the realm of computer vision research. This paper presents an innovative network called the Dual-Direction Attention Mixed Feature Network (DDAMFN) specifically designed for FER, boasting both robustness and lightweight characteristics. The network architecture comprises two primary components: the Mixed Feature Network (MFN) serving as the backbone, and the Dual-Direction Attention Network (DDAN) functioning as the head. To enhance the network&rsquo;s capability in the MFN, resilient features are extracted by utilizing mixed-size kernels. Additionally, a new Dual-Direction Attention (DDA) head that generates attention maps in two orientations is proposed, enabling the model to capture long-range dependencies effectively. To further improve the accuracy, a novel attention loss mechanism for the DDAN is introduced with different heads focusing on distinct areas of the input. Experimental evaluations on several widely used public datasets, including AffectNet, RAF-DB, and FERPlus, demonstrate the superiority of the DDAMFN compared to other existing models, which establishes that the DDAMFN as the state-of-the-art model in the field of FER.},
DOI = {10.3390/electronics12173595}
}

@misc{chen2023static,
      title={From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial Expression Recognition in Videos}, 
      author={Yin Chen and Jia Li and Shiguang Shan and Meng Wang and Richang Hong},
      year={2023},
      eprint={2312.05447},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{mao2023poster,
      title={POSTER++: A simpler and stronger facial expression recognition network}, 
      author={Jiawei Mao and Rui Xu and Xuesong Yin and Yuanqi Chang and Binling Nie and Aibin Huang},
      year={2023},
      eprint={2301.12149},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{9815154,
  author={Savchenko, Andrey V. and Savchenko, Lyudmila V. and Makarov, Ilya},
  journal={IEEE Transactions on Affective Computing}, 
  title={Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network}, 
  year={2022},
  volume={13},
  number={4},
  pages={2132-2143},
  keywords={Feature extraction;Face recognition;Electronic learning;Emotion recognition;Task analysis;Training;Convolutional neural networks;Online learning;e-learning;video-based facial expression recognition;engagement prediction;group-level emotion recognition;mobile devices},
  doi={10.1109/TAFFC.2022.3188390}}

@misc{kollias2019expression,
      title={Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace}, 
      author={Dimitrios Kollias and Stefanos Zafeiriou},
      year={2019},
      eprint={1910.04855},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{RYUMINA2022435,
title = {In search of a robust facial expressions recognition model: A large-scale visual cross-corpus study},
journal = {Neurocomputing},
volume = {514},
pages = {435-450},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222012656},
author = {Elena Ryumina and Denis Dresvyanskiy and Alexey Karpov},
keywords = {Visual emotion recognition, Affective computing, Paralinguistic analysis, Cross-corpus analysis, Deep learning, End-to-end model},
abstract = {Many researchers have been seeking robust emotion recognition system for already last two decades. It would advance computer systems to a new level of interaction, providing much more natural feedback during human–computer interaction due to analysis of user affect state. However, one of the key problems in this domain is a lack of generalization ability: we observe dramatic degradation of model performance when it was trained on one corpus and evaluated on another one. Although some studies were done in this direction, visual modality still remains under-investigated. Therefore, we introduce the visual cross-corpus study conducted with the utilization of eight corpora, which differ in recording conditions, participants’ appearance characteristics, and complexity of data processing. We propose a visual-based end-to-end emotion recognition framework, which consists of the robust pre-trained backbone model and temporal sub-system in order to model temporal dependencies across many video frames. In addition, a detailed analysis of mistakes and advantages of the backbone model is provided, demonstrating its high ability of generalization. Our results show that the backbone model has achieved the accuracy of 66.4% on the AffectNet dataset, outperforming all the state-of-the-art results. Moreover, the CNN-LSTM model has demonstrated a decent efficacy on dynamic visual datasets during cross-corpus experiments, achieving comparable with state-of-the-art results. In addition, we provide backbone and CNN-LSTM models for future researchers: they can be accessed via GitHub.}
}

@inproceedings{Antoniadis_2021,
   title={Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition},
   url={http://dx.doi.org/10.1109/FG52635.2021.9667014},
   DOI={10.1109/fg52635.2021.9667014},
   booktitle={2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)},
   publisher={IEEE},
   author={Antoniadis, Panagiotis and Filntisis, Panagiotis Paraskevas and Maragos, Petros},
   year={2021},
   month=dec }

@misc{tu2022maxvit,
      title={MaxViT: Multi-Axis Vision Transformer}, 
      author={Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Bovik and Yinxiao Li},
      year={2022},
      eprint={2204.01697},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}